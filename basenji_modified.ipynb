{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import FASTA file (Get hg38 fasta file)\n",
    "import os, subprocess\n",
    "\n",
    "if not os.path.isfile('data/hg19.ml.fa'):\n",
    "    subprocess.call('curl -o data/hg19.ml.fa https://storage.googleapis.com/basenji_tutorial_data/hg19.ml.fa', shell=True)\n",
    "    subprocess.call('curl -o data/hg19.ml.fa.fai https://storage.googleapis.com/basenji_tutorial_data/hg19.ml.fa.fai', shell=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pre-processed data (if it is not processed, you might want to run bam_cov.py on bam file to distribute multi-mapping reads and estimate genomic coverage)\n",
    "\n",
    "if not os.path.isfile('data/CNhs11760.bw'):\n",
    "    subprocess.call('curl -o data/CNhs11760.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs11760.bw', shell=True)\n",
    "    subprocess.call('curl -o data/CNhs12843.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs12843.bw', shell=True)\n",
    "    subprocess.call('curl -o data/CNhs12856.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs12856.bw', shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " import basenji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pysam.libcalignmentfile.AlignmentFile object at 0x7fcfc53a88b0>\n",
      "/gpfs/data/shared/databases/refchef_refs/grch38_release98/primary/Homo_sapiens.GRCh38.dna.primary_assembly.fa\n"
     ]
    }
   ],
   "source": [
    "#Import bigwigfile \n",
    "#import pysam\n",
    "#bamfile = pysam.AlignmentFile(\"/users/ysuita/data/ysuita/basenji/data/GSC1-stem.bam\", \"rb\")\n",
    "#print(bamfile)\n",
    "Bigwig=\"\"\n",
    "\n",
    "#Import hg38 fasta file\n",
    "from pysam import FastaFile\n",
    "fasta=\"/gpfs/data/shared/databases/refchef_refs/grch38_release98/primary/Homo_sapiens.GRCh38.dna.primary_assembly.fa\"\n",
    " # read FASTA file\n",
    "#sequences_object = FastaFile(fasta)\n",
    "print(fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out the above bigwig files and lables to a sample table\n",
    "\n",
    "lines = [['index','identifier','file','clip','sum_stat','description']]\n",
    "lines.append(['0', 'CNhs11760', 'data/CNhs11760.bw', '384', 'sum', 'aorta'])\n",
    "lines.append(['1', 'CNhs12843', 'data/CNhs12843.bw', '384', 'sum', 'artery'])\n",
    "lines.append(['2', 'CNhs12856', 'data/CNhs12856.bw', '384', 'sum', 'pulmonic_valve'])\n",
    "\n",
    "samples_out = open('data/heart_wigs.txt', 'w')\n",
    "for line in lines:\n",
    "    print('\\t'.join(line), file=samples_out)\n",
    "samples_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride_train 1 converted to 131072.000000\n",
      "stride_test 1 converted to 131072.000000\n",
      "Contigs divided into\n",
      " Train:  4701 contigs, 2169074921 nt (0.8005)\n",
      " Valid:   572 contigs,  270358978 nt (0.0998)\n",
      " Test:    584 contigs,  270330829 nt (0.0998)\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_read.py -w 128 -u sum -c 384.000000 -s 1.000000 data/CNhs11760.bw /users/ysuita/data/ysuita/basenji/data/heart_l131k/sequences.bed /users/ysuita/data/ysuita/basenji/data/heart_l131k/seqs_cov/0.h5\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_read.py -w 128 -u sum -c 384.000000 -s 1.000000 data/CNhs12843.bw /users/ysuita/data/ysuita/basenji/data/heart_l131k/sequences.bed /users/ysuita/data/ysuita/basenji/data/heart_l131k/seqs_cov/1.h5\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_read.py -w 128 -u sum -c 384.000000 -s 1.000000 data/CNhs12856.bw /users/ysuita/data/ysuita/basenji/data/heart_l131k/sequences.bed /users/ysuita/data/ysuita/basenji/data/heart_l131k/seqs_cov/2.h5\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py -s 0 -e 256 --umap_clip 1.000000 -x 0 /users/ysuita/data/ysuita/basenji/data/hg19.ml.fa /users/ysuita/data/ysuita/basenji/data/heart_l131k/sequences.bed /users/ysuita/data/ysuita/basenji/data/heart_l131k/seqs_cov /users/ysuita/data/ysuita/basenji/data/heart_l131k/tfrecords/train-0.tfr\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py -s 256 -e 512 --umap_clip 1.000000 -x 0 /users/ysuita/data/ysuita/basenji/data/hg19.ml.fa /users/ysuita/data/ysuita/basenji/data/heart_l131k/sequences.bed /users/ysuita/data/ysuita/basenji/data/heart_l131k/seqs_cov /users/ysuita/data/ysuita/basenji/data/heart_l131k/tfrecords/train-1.tfr\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py -s 512 -e 768 --umap_clip 1.000000 -x 0 /users/ysuita/data/ysuita/basenji/data/hg19.ml.fa /users/ysuita/data/ysuita/basenji/data/heart_l131k/sequences.bed /users/ysuita/data/ysuita/basenji/data/heart_l131k/seqs_cov /users/ysuita/data/ysuita/basenji/data/heart_l131k/tfrecords/train-2.tfr\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py -s 768 -e 1024 --umap_clip 1.000000 -x 0 /users/ysuita/data/ysuita/basenji/data/hg19.ml.fa /users/ysuita/data/ysuita/basenji/data/heart_l131k/sequences.bed /users/ysuita/data/ysuita/basenji/data/heart_l131k/seqs_cov /users/ysuita/data/ysuita/basenji/data/heart_l131k/tfrecords/train-3.tfr\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py -s 1024 -e 1280 --umap_clip 1.000000 -x 0 /users/ysuita/data/ysuita/basenji/data/hg19.ml.fa /users/ysuita/data/ysuita/basenji/data/heart_l131k/sequences.bed /users/ysuita/data/ysuita/basenji/data/heart_l131k/seqs_cov /users/ysuita/data/ysuita/basenji/data/heart_l131k/tfrecords/train-4.tfr\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py -s 1280 -e 1499 --umap_clip 1.000000 -x 0 /users/ysuita/data/ysuita/basenji/data/hg19.ml.fa /users/ysuita/data/ysuita/basenji/data/heart_l131k/sequences.bed /users/ysuita/data/ysuita/basenji/data/heart_l131k/seqs_cov /users/ysuita/data/ysuita/basenji/data/heart_l131k/tfrecords/train-5.tfr\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py -s 1499 -e 1679 --umap_clip 1.000000 -x 0 /users/ysuita/data/ysuita/basenji/data/hg19.ml.fa /users/ysuita/data/ysuita/basenji/data/heart_l131k/sequences.bed /users/ysuita/data/ysuita/basenji/data/heart_l131k/seqs_cov /users/ysuita/data/ysuita/basenji/data/heart_l131k/tfrecords/valid-0.tfr\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py -s 1679 -e 1858 --umap_clip 1.000000 -x 0 /users/ysuita/data/ysuita/basenji/data/hg19.ml.fa /users/ysuita/data/ysuita/basenji/data/heart_l131k/sequences.bed /users/ysuita/data/ysuita/basenji/data/heart_l131k/seqs_cov /users/ysuita/data/ysuita/basenji/data/heart_l131k/tfrecords/test-0.tfr\n",
      "2022-07-26 15:19:30.149903: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/runtime/opt/gcc/8.3/lib64:/gpfs/runtime/opt/gsl/2.5/lib:/gpfs/runtime/opt/boost/1.69/lib:/gpfs/runtime/opt/R/4.0.0/lib64/R/lib:/gpfs/runtime/opt/pcre2/10.35/lib:/gpfs/runtime/opt/python/3.8.12_gcc8.3/lib:/gpfs/runtime/opt/intel/2017.0/lib/intel64:/gpfs/runtime/opt/intel/2017.0/mkl/lib/intel64:/gpfs/runtime/opt/java/8u111/jre/lib/amd64\n",
      "2022-07-26 15:19:30.149934: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/runtime/opt/gcc/8.3/lib64:/gpfs/runtime/opt/gsl/2.5/lib:/gpfs/runtime/opt/boost/1.69/lib:/gpfs/runtime/opt/R/4.0.0/lib64/R/lib:/gpfs/runtime/opt/pcre2/10.35/lib:/gpfs/runtime/opt/python/3.8.12_gcc8.3/lib:/gpfs/runtime/opt/intel/2017.0/lib/intel64:/gpfs/runtime/opt/intel/2017.0/mkl/lib/intel64:/gpfs/runtime/opt/java/8u111/jre/lib/amd64\n",
      "2022-07-26 15:19:30.149919: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/runtime/opt/gcc/8.3/lib64:/gpfs/runtime/opt/gsl/2.5/lib:/gpfs/runtime/opt/boost/1.69/lib:/gpfs/runtime/opt/R/4.0.0/lib64/R/lib:/gpfs/runtime/opt/pcre2/10.35/lib:/gpfs/runtime/opt/python/3.8.12_gcc8.3/lib:/gpfs/runtime/opt/intel/2017.0/lib/intel64:/gpfs/runtime/opt/intel/2017.0/mkl/lib/intel64:/gpfs/runtime/opt/java/8u111/jre/lib/amd64\n",
      "2022-07-26 15:19:30.149953: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/runtime/opt/gcc/8.3/lib64:/gpfs/runtime/opt/gsl/2.5/lib:/gpfs/runtime/opt/boost/1.69/lib:/gpfs/runtime/opt/R/4.0.0/lib64/R/lib:/gpfs/runtime/opt/pcre2/10.35/lib:/gpfs/runtime/opt/python/3.8.12_gcc8.3/lib:/gpfs/runtime/opt/intel/2017.0/lib/intel64:/gpfs/runtime/opt/intel/2017.0/mkl/lib/intel64:/gpfs/runtime/opt/java/8u111/jre/lib/amd64\n",
      "2022-07-26 15:19:30.149905: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/runtime/opt/gcc/8.3/lib64:/gpfs/runtime/opt/gsl/2.5/lib:/gpfs/runtime/opt/boost/1.69/lib:/gpfs/runtime/opt/R/4.0.0/lib64/R/lib:/gpfs/runtime/opt/pcre2/10.35/lib:/gpfs/runtime/opt/python/3.8.12_gcc8.3/lib:/gpfs/runtime/opt/intel/2017.0/lib/intel64:/gpfs/runtime/opt/intel/2017.0/mkl/lib/intel64:/gpfs/runtime/opt/java/8u111/jre/lib/amd64\n",
      "2022-07-26 15:19:30.149905: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/runtime/opt/gcc/8.3/lib64:/gpfs/runtime/opt/gsl/2.5/lib:/gpfs/runtime/opt/boost/1.69/lib:/gpfs/runtime/opt/R/4.0.0/lib64/R/lib:/gpfs/runtime/opt/pcre2/10.35/lib:/gpfs/runtime/opt/python/3.8.12_gcc8.3/lib:/gpfs/runtime/opt/intel/2017.0/lib/intel64:/gpfs/runtime/opt/intel/2017.0/mkl/lib/intel64:/gpfs/runtime/opt/java/8u111/jre/lib/amd64\n",
      "2022-07-26 15:19:30.150004: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-26 15:19:30.149923: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/runtime/opt/gcc/8.3/lib64:/gpfs/runtime/opt/gsl/2.5/lib:/gpfs/runtime/opt/boost/1.69/lib:/gpfs/runtime/opt/R/4.0.0/lib64/R/lib:/gpfs/runtime/opt/pcre2/10.35/lib:/gpfs/runtime/opt/python/3.8.12_gcc8.3/lib:/gpfs/runtime/opt/intel/2017.0/lib/intel64:/gpfs/runtime/opt/intel/2017.0/mkl/lib/intel64:/gpfs/runtime/opt/java/8u111/jre/lib/amd64\n",
      "2022-07-26 15:19:30.149945: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/runtime/opt/gcc/8.3/lib64:/gpfs/runtime/opt/gsl/2.5/lib:/gpfs/runtime/opt/boost/1.69/lib:/gpfs/runtime/opt/R/4.0.0/lib64/R/lib:/gpfs/runtime/opt/pcre2/10.35/lib:/gpfs/runtime/opt/python/3.8.12_gcc8.3/lib:/gpfs/runtime/opt/intel/2017.0/lib/intel64:/gpfs/runtime/opt/intel/2017.0/mkl/lib/intel64:/gpfs/runtime/opt/java/8u111/jre/lib/amd64\n",
      "2022-07-26 15:19:30.149966: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-26 15:19:30.149972: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-26 15:19:30.150037: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-26 15:19:30.150041: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-26 15:19:30.149984: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-26 15:19:30.149994: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-26 15:19:30.150014: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py:201: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  values = values.flatten().tostring()\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py:201: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  values = values.flatten().tostring()\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py:201: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  values = values.flatten().tostring()\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py:201: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  values = values.flatten().tostring()\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py:201: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  values = values.flatten().tostring()\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py:201: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  values = values.flatten().tostring()\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py:201: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  values = values.flatten().tostring()\n",
      "/users/ysuita/data/ysuita/basenji/bin/basenji_data_write.py:201: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  values = values.flatten().tostring()\n"
     ]
    }
   ],
   "source": [
    "#Run model - choose genomic sequences to form batchs for stochastic gradient descent, devide them into training (80%)/valudation(10%)/test sets(10%), and construct TFRecords to provide to downstream programs\n",
    "#Try 70% / 15% / 15% \n",
    "import basenji\n",
    "! bin/basenji_data.py -d .1 -g /users/ysuita/data/ysuita/basenji/tutorials/data/unmap_macro.bed -l 131072 --local -o /users/ysuita/data/ysuita/basenji/data/heart_l131k -p 8 -t .1 -v .1 -w 128 /users/ysuita/data/ysuita/basenji/data/hg19.ml.fa /users/ysuita/data/ysuita/basenji/data/heart_wigs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    179 test\n",
      "   1499 train\n",
      "    180 valid\n"
     ]
    }
   ],
   "source": [
    "#To confirm data/heart_l131k contains relevant data for training\n",
    "! cut -f4 data/heart_l131k/sequences.bed | sort | uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr2\t140425791\t140556863\ttrain\n",
      "chr16\t27143973\t27275045\ttrain\n",
      "chr14\t72972403\t73103475\ttrain\n",
      "chr11\t116271608\t116402680\tvalid\n",
      "chr2\t26239628\t26370700\tvalid\n",
      "chr6\t102241368\t102372440\tvalid\n",
      "chr14\t76007464\t76138536\ttest\n",
      "chr13\t99391330\t99522402\ttest\n",
      "chr15\t52280212\t52411284\ttest\n",
      "-rw-r--r-- 1 ysuita ntapinos 10609221 Jul 26 15:19 data/heart_l131k/tfrecords/test-0.tfr\n",
      "-rw-r--r-- 1 ysuita ntapinos 15168689 Jul 26 15:20 data/heart_l131k/tfrecords/train-0.tfr\n",
      "-rw-r--r-- 1 ysuita ntapinos 15153640 Jul 26 15:20 data/heart_l131k/tfrecords/train-1.tfr\n",
      "-rw-r--r-- 1 ysuita ntapinos 15175334 Jul 26 15:20 data/heart_l131k/tfrecords/train-2.tfr\n",
      "-rw-r--r-- 1 ysuita ntapinos 15181231 Jul 26 15:20 data/heart_l131k/tfrecords/train-3.tfr\n",
      "-rw-r--r-- 1 ysuita ntapinos 15151442 Jul 26 15:20 data/heart_l131k/tfrecords/train-4.tfr\n",
      "-rw-r--r-- 1 ysuita ntapinos 12976797 Jul 26 15:19 data/heart_l131k/tfrecords/train-5.tfr\n",
      "-rw-r--r-- 1 ysuita ntapinos 10661284 Jul 26 15:19 data/heart_l131k/tfrecords/valid-0.tfr\n"
     ]
    }
   ],
   "source": [
    "#To confirm sequences.bed contains the train/valid/test sequences.\n",
    "! head -n3 data/heart_l131k/sequences.bed\n",
    "! grep valid data/heart_l131k/sequences.bed | head -n3\n",
    "! grep test data/heart_l131k/sequences.bed | head -n3\n",
    "! ls -l data/heart_l131k/tfrecords/*.tfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For training, grab HDF5 file which contains 10% of training sequences\n",
    "\n",
    "import glob, os, subprocess\n",
    "from IPython.display import IFrame\n",
    "\n",
    "if len(glob.glob('data/heart_l131k/tfrecords/*.tfr')) == 0:\n",
    "    subprocess.call('curl -o data/heart_l131k.tgz https://storage.googleapis.com/basenji_tutorial_data/heart_l131k.tgz', shell=True)\n",
    "    subprocess.call('tar -xzvf data/heart_l131k.tgz', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-26 16:05:06.065344: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/runtime/opt/gcc/8.3/lib64:/gpfs/runtime/opt/gsl/2.5/lib:/gpfs/runtime/opt/boost/1.69/lib:/gpfs/runtime/opt/R/4.0.0/lib64/R/lib:/gpfs/runtime/opt/pcre2/10.35/lib:/gpfs/runtime/opt/python/3.8.12_gcc8.3/lib:/gpfs/runtime/opt/intel/2017.0/lib/intel64:/gpfs/runtime/opt/intel/2017.0/mkl/lib/intel64:/gpfs/runtime/opt/java/8u111/jre/lib/amd64\n",
      "2022-07-26 16:05:06.065391: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-26 16:05:15.528435: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/runtime/opt/gcc/8.3/lib64:/gpfs/runtime/opt/gsl/2.5/lib:/gpfs/runtime/opt/boost/1.69/lib:/gpfs/runtime/opt/R/4.0.0/lib64/R/lib:/gpfs/runtime/opt/pcre2/10.35/lib:/gpfs/runtime/opt/python/3.8.12_gcc8.3/lib:/gpfs/runtime/opt/intel/2017.0/lib/intel64:/gpfs/runtime/opt/intel/2017.0/mkl/lib/intel64:/gpfs/runtime/opt/java/8u111/jre/lib/amd64\n",
      "2022-07-26 16:05:15.528516: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-26 16:05:15.528545: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (login005.oscar.ccv.brown.edu): /proc/driver/nvidia/version does not exist\n",
      "2022-07-26 16:05:15.529129: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 131072, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 131072, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 131072, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 131072, 4)    0           ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 131072, 64)   3840        ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 131072, 64)  256         ['conv1d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 16384, 64)    0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 16384, 64)    0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 16384, 64)    20480       ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 16384, 64)   256         ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 4096, 64)    0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 4096, 64)     0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 4096, 72)     23040       ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 4096, 72)    288         ['conv1d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 1024, 72)    0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 1024, 72)     0           ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 1024, 32)     6912        ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 1024, 32)    128         ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 1024, 32)     0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 1024, 72)     2304        ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 1024, 72)    288         ['conv1d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024, 72)     0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 1024, 72)     0           ['max_pooling1d_2[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 1024, 72)     0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 1024, 32)     6912        ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 1024, 32)    128         ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 1024, 32)     0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 1024, 72)     2304        ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 1024, 72)    288         ['conv1d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 1024, 72)     0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 1024, 72)     0           ['add[0][0]',                    \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 1024, 72)     0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 1024, 32)     6912        ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 1024, 32)    128         ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 1024, 32)     0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 1024, 72)     2304        ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 1024, 72)    288         ['conv1d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 1024, 72)     0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 1024, 72)     0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 1024, 72)     0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 1024, 32)     6912        ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 1024, 32)    128         ['conv1d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 1024, 32)     0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 1024, 72)     2304        ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 1024, 72)    288         ['conv1d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 1024, 72)     0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 1024, 72)     0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 1024, 72)     0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 1024, 32)     6912        ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 1024, 32)    128         ['conv1d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.nn.gelu_12 (TFOpLambda)     (None, 1024, 32)     0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 1024, 72)     2304        ['tf.nn.gelu_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 1024, 72)    288         ['conv1d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 1024, 72)     0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 1024, 72)     0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " tf.nn.gelu_13 (TFOpLambda)     (None, 1024, 72)     0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 1024, 32)     6912        ['tf.nn.gelu_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 1024, 32)    128         ['conv1d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.nn.gelu_14 (TFOpLambda)     (None, 1024, 32)     0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 1024, 72)     2304        ['tf.nn.gelu_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 1024, 72)    288         ['conv1d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 1024, 72)     0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 1024, 72)     0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " tf.nn.gelu_15 (TFOpLambda)     (None, 1024, 72)     0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 1024, 64)     4608        ['tf.nn.gelu_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 1024, 64)    256         ['conv1d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 1024, 64)     0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " tf.nn.gelu_16 (TFOpLambda)     (None, 1024, 64)     0           ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024, 3)      195         ['tf.nn.gelu_16[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 1024, 3)     0           ['dense[0][0]',                  \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 111,011\n",
      "Trainable params: 109,235\n",
      "Non-trainable params: 1,776\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [128]\n",
      "target_lengths [1024]\n",
      "target_crops [0]\n",
      "No checkpoints found.\n",
      "Epoch 0 - 190s - train_loss: 0.4123 - train_r: 0.2971 - train_r2: 0.0544 - valid_loss: 0.3691 - valid_r: 0.3698 - valid_r2: 0.1283 - best!\n",
      "Epoch 1 - 183s - train_loss: 0.3656 - train_r: 0.3968 - train_r2: 0.1582 - valid_loss: 0.3677 - valid_r: 0.3974 - valid_r2: 0.1203 - best!\n",
      "Epoch 2 - 184s - train_loss: 0.3594 - train_r: 0.4084 - train_r2: 0.1671 - valid_loss: 0.3532 - valid_r: 0.4274 - valid_r2: 0.1762 - best!\n",
      "Epoch 3 - 183s - train_loss: 0.3506 - train_r: 0.4446 - train_r2: 0.2004 - valid_loss: 0.3470 - valid_r: 0.4381 - valid_r2: 0.1936 - best!\n",
      "Epoch 4 - 182s - train_loss: 0.3504 - train_r: 0.4384 - train_r2: 0.1920 - valid_loss: 0.3466 - valid_r: 0.4408 - valid_r2: 0.1845 - best!\n",
      "Epoch 5 - 182s - train_loss: 0.3425 - train_r: 0.4713 - train_r2: 0.2246 - valid_loss: 0.3462 - valid_r: 0.4589 - valid_r2: 0.1823 - best!\n",
      "Epoch 6 - 183s - train_loss: 0.3368 - train_r: 0.4920 - train_r2: 0.2447 - valid_loss: 0.3410 - valid_r: 0.4750 - valid_r2: 0.1931 - best!\n"
     ]
    }
   ],
   "source": [
    "#For Training\n",
    "output=\"/users/ysuita/data/ysuita/basenji/tutorials/models/heart\"\n",
    "json=\"/users/ysuita/data/ysuita/basenji/tutorials/models/params_small.json\"\n",
    "data_dir=\"/users/ysuita/data/ysuita/basenji/data/heart_l131k\"\n",
    "! /users/ysuita/data/ysuita/basenji/bin/basenji_train.py -o  output json data_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': {'batch_size': 4, 'loss': 'poisson', 'optimizer': 'sgd', 'learning_rate': 0.1, 'momentum': 0.99, 'train_epochs': 20, 'patience': 8, 'clip_norm': 2}, 'model': {'seq_length': 131072, 'augment_rc': True, 'augment_shift': 3, 'activation': 'gelu', 'norm_type': 'batch', 'bn_momentum': 0.9, 'trunk': [{'name': 'conv_block', 'filters': 64, 'kernel_size': 15, 'pool_size': 8}, {'name': 'conv_tower', 'filters_init': 64, 'filters_mult': 1.125, 'kernel_size': 5, 'pool_size': 4, 'repeat': 2}, {'name': 'dilated_residual', 'filters': 32, 'rate_mult': 2, 'repeat': 6, 'dropout': 0.25}, {'name': 'conv_block', 'filters': 64, 'dropout': 0.05}], 'head': {'name': 'final', 'units': 3, 'activation': 'softplus'}}}\n",
      "<HDF5 file \"model_best.h5\" (mode r)>\n",
      "/bin/bash: nvcc: command not found\n",
      "2022-07-28 16:54:08.861605: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/runtime/opt/gcc/8.3/lib64:/gpfs/runtime/opt/gsl/2.5/lib:/gpfs/runtime/opt/boost/1.69/lib:/gpfs/runtime/opt/R/4.0.0/lib64/R/lib:/gpfs/runtime/opt/pcre2/10.35/lib:/gpfs/runtime/opt/python/3.8.12_gcc8.3/lib:/gpfs/runtime/opt/intel/2017.0/lib/intel64:/gpfs/runtime/opt/intel/2017.0/mkl/lib/intel64:/gpfs/runtime/opt/java/8u111/jre/lib/amd64\n",
      "2022-07-28 16:54:08.861648: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/ysuita/data/ysuita/basenji/bin/basenji_test.py\", line 33, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/tensorflow/__init__.py\", line 37, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/tensorflow/python/__init__.py\", line 42, in <module>\n",
      "    from tensorflow.python import data\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/tensorflow/python/data/__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python.data import experimental\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/tensorflow/python/data/experimental/__init__.py\", line 124, in <module>\n",
      "    from tensorflow.python.data.experimental.ops.parsing_ops import parse_example_dataset\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/parsing_ops.py\", line 22, in <module>\n",
      "    from tensorflow.python.ops import parsing_ops\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/tensorflow/python/ops/parsing_ops.py\", line 23, in <module>\n",
      "    from tensorflow.python.ops import parsing_config\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/tensorflow/python/ops/parsing_config.py\", line 28, in <module>\n",
      "    from tensorflow.python.ops.ragged import ragged_math_ops\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/tensorflow/python/ops/ragged/ragged_math_ops.py\", line 27, in <module>\n",
      "    from tensorflow.python.ops import map_fn\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/tensorflow/python/ops/map_fn.py\", line 21, in <module>\n",
      "    from tensorflow.python.autograph.core import ag_ctx as autograph_ctx\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/tensorflow/python/autograph/__init__.py\", line 35, in <module>\n",
      "    from tensorflow.python.autograph.core.converter import ConversionOptions\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/tensorflow/python/autograph/core/converter.py\", line 64, in <module>\n",
      "    from tensorflow.python.autograph.pyct import anno\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/anno.py\", line 24, in <module>\n",
      "    import gast\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/gast/__init__.py\", line 1, in <module>\n",
      "    from .gast import *\n",
      "  File \"/gpfs/data/ntapinos/ysuita/lib/python3.10/site-packages/gast/gast.py\", line 296, in <module>\n",
      "    from .ast3 import ast_to_gast, gast_to_ast\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 975, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1074, in get_data\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#For testing gene expression precitions based on the trained model\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "\n",
    "\n",
    "test_parameters=\"/users/ysuita/data/ysuita/basenji/tutorials/models/params_small.json\"\n",
    "#Read model parameters \n",
    "with open(test_parameters) as params_open:\n",
    " test_params=json.load(params_open)\n",
    "print(test_params)\n",
    "\n",
    "trained_model=\"/users/ysuita/data/ysuita/basenji/tutorials/models/heart/model_best.h5\"\n",
    "#Read trained model\n",
    "with open(trained_model) as train: \n",
    " tra_model=h5py.File(trained_model,'r')\n",
    "print(tra_model)\n",
    "\n",
    "test_output=\"/users/ysuita/data/ysuita/basenji/tutorials/models/heart_test\"\n",
    "test_inputdata=\"/users/ysuita/data/ysuita/basenji/data/heart_l131k\"\n",
    "\n",
    "#! echo testing\n",
    "! nvcc --version\n",
    "#! /users/ysuita/data/ysuita/basenji/bin/basenji_test.py --ai 0,1,2 -o test_output --rc --shifts \"1,0,-1\" test_params train.tra_model test_inputdata\n",
    "! /users/ysuita/data/ysuita/basenji/bin/basenji_test.py --ai 0,1,2 -o /users/ysuita/data/ysuita/basenji/tutorials/models/heart_test --rc --shifts \"1,0,-1\" test_params train.tra_model /users/ysuita/data/ysuita/basenji/data/heart_l131k\n",
    "\n",
    "\n",
    "#print(test_targets_ti_log)\n",
    "#print(test_preds_ti_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              x         y\n",
      "0      0.245256  0.223097\n",
      "1      0.000000  0.049721\n",
      "2      0.449407  0.077233\n",
      "3      0.000000  0.131214\n",
      "4      0.000000  0.165300\n",
      "...         ...       ...\n",
      "22907  0.000000  0.451469\n",
      "22908  0.000000  0.664447\n",
      "22909  0.000000  1.041659\n",
      "22910  1.128639  1.758223\n",
      "22911  0.000000  0.318966\n",
      "\n",
      "[22912 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "regplot() got an unexpected keyword argument 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m z\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/users/ysuita/data/ysuita/basenji/data/z.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(z)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mplots\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mpoly_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: regplot() got an unexpected keyword argument 'x'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from basenji import plots\n",
    "z=pd.read_pickle(\"/users/ysuita/data/ysuita/basenji/data/z.pkl\")\n",
    "print(z)\n",
    "plots.regplot(\n",
    "          x='x',\n",
    "          y='y',\n",
    "          data=z,\n",
    "          poly_order=1,\n",
    "          alpha=0.3,\n",
    "          sample=500)\n",
    "\n",
    "          #figsize=(6, 6),\n",
    "          #x_label='log2 Experiment',\n",
    "          #y_label='log2 Prediction',\n",
    "          #table=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "791a560f92be729cf63c50c1d13a4de828932d8bdf7299e018776c32980abb71"
  },
  "kernelspec": {
   "display_name": "env_basenji",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
